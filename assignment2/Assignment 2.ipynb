{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax test 1 passed!\n",
      "Softmax test 2 passed!\n",
      "Basic (non-exhaustive) softmax tests pass\n",
      "\n",
      "Cross-entropy test 1 passed!\n",
      "Basic (non-exhaustive) cross-entropy tests pass\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.general_utils import test_all_close\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function in tensorflow.\n",
    "\n",
    "    You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "    tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "    not need to use all of these functions). Recall also that many common\n",
    "    tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n",
    "    if x and y are both tensors). Make sure to implement the numerical stability\n",
    "    fixes as in the previous homework!\n",
    "\n",
    "    Args:\n",
    "        x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "                  represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "                  input as in the previous homework)\n",
    "    Returns:\n",
    "        out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "                  tensor in this problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    x_max = tf.reduce_max(x, axis=1, keep_dims=True)\n",
    "    x_norm = x - x_max\n",
    "    exp_x_norm = tf.exp(x_norm)\n",
    "    row_sum = tf.reduce_sum(exp_x_norm, axis=1, keep_dims=True)\n",
    "    out = exp_x_norm / row_sum\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y, yhat):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss in tensorflow.\n",
    "    The loss should be summed over the current minibatch.\n",
    "\n",
    "    y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "    of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "    be of dtype tf.float32.\n",
    "\n",
    "    The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
    "    solutions are possible, so you may not need to use all of these functions).\n",
    "\n",
    "    Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
    "                functions.\n",
    "\n",
    "    Args:\n",
    "        y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
    "        yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
    "                    probability distribution and should sum to 1.\n",
    "    Returns:\n",
    "        out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
    "                    tensor in the problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    out = tf.reduce_sum(-tf.log(tf.boolean_mask(yhat, mask=tf.cast(y, dtype=tf.bool))))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of softmax to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "\n",
    "    test1 = softmax(tf.constant(np.array([[1001, 1002], [3, 4]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "            test1 = sess.run(test1)\n",
    "    test_all_close(\"Softmax test 1\", test1, np.array([[0.26894142,  0.73105858],\n",
    "                                                      [0.26894142,  0.73105858]]))\n",
    "\n",
    "    test2 = softmax(tf.constant(np.array([[-1001, -1002]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "            test2 = sess.run(test2)\n",
    "    test_all_close(\"Softmax test 2\", test2, np.array([[0.73105858, 0.26894142]]))\n",
    "\n",
    "    print(\"Basic (non-exhaustive) softmax tests pass\\n\")\n",
    "\n",
    "\n",
    "def test_cross_entropy_loss_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of cross_entropy_loss to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "    yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "    test1 = cross_entropy_loss(\n",
    "            tf.constant(y, dtype=tf.int32),\n",
    "            tf.constant(yhat, dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        test1 = sess.run(test1)\n",
    "    expected = -3 * np.log(.5)\n",
    "    test_all_close(\"Cross-entropy test 1\", test1, expected)\n",
    "\n",
    "    print (\"Basic (non-exhaustive) cross-entropy tests pass\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_softmax_basic()\n",
    "    test_cross_entropy_loss_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "a.pop(-1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'wo cao ni da ye'\n",
    "A = s.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = A[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wo', 'cao', 'ni', 'da', 'ye']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] = 'shit'\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shit', 'ni', 'da', 'ye']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = list(range(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a[1] = 'wocao'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 'wocao', 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-e5ec68bc84d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "a[[1, 2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-GPU(py3)",
   "language": "python",
   "name": "tensorflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
